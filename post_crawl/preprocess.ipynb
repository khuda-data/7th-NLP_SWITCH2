{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í¬ë¡¤ë§ ëª¨ë“ˆ import\n",
        "from wevity_crawler import crawl_wevity\n",
        "from dacon_crawler import crawl_dacon\n",
        "from devEvent_crawler import crawl_devEvent\n",
        "from inflearn_crawler import crawl_inflearn\n",
        "\n",
        "# ê° ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° í¬ë¡¤ë§\n",
        "df_wevity = crawl_wevity()\n",
        "df_dev = crawl_devEvent()\n",
        "df_inflearn = crawl_inflearn()\n",
        "df_dacon = crawl_dacon(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ë°ì´í„° í†µí•©\n",
        "df = pd.concat([df_wevity, df_dacon, df_dev, df_inflearn], ignore_index=True)\n",
        "print(f\"ì´ {len(df)}ê°œ ì´ë²¤íŠ¸ í†µí•©ë¨\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•œ ë°ì´í„° ì „ì²˜ë¦¬\n",
        "import re\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "def normalize_date(date_str: str) -> str:\n",
        "    \"\"\"\n",
        "    ë‚ ì§œ ë¬¸ìì—´ì„ 'YYYY.MM.DD ~ YYYY.MM.DD' í˜•ì‹ìœ¼ë¡œ í†µì¼í•œë‹¤.\n",
        "\n",
        "    ê·œì¹™\n",
        "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    1. ê´„í˜¸ ì•ˆ ì •ë³´Â·ì‹œê°„(HH:MM, HHMM)Â·ë¶ˆí•„ìš” ê³µë°± ì œê±°\n",
        "    2. 'YYYY.MM.DD ~ YYYY.MM.DD'  â†’ ê·¸ëŒ€ë¡œ í¬ë§·\n",
        "    3. 'YYYY.MM.DD ~ MM.DD'       â†’ ë’¤ìª½ ì—°ë„ = ì•ìª½ ì—°ë„\n",
        "    4. 'YYYY.MM.DD â€¦ ~ HH:MM'     â†’ ê°™ì€ ë‚  ë²”ìœ„\n",
        "    5. ë‹¨ì¼ ë‚ ì§œ(YYYY.MM.DD)      â†’ ì•ë’¤ ë‚ ì§œ ë™ì¼\n",
        "    \"\"\"\n",
        "    date_str = str(date_str).strip()\n",
        "\n",
        "    # â”€â”€ (1) ë…¸ì´ì¦ˆ ì œê±° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    date_str = re.sub(r'\\(.*?\\)', '', date_str)               # (ìš”ì¼) (í•œêµ­ í‘œì¤€ì‹œ)\n",
        "    date_str = re.sub(r'\\b\\d{1,2}:\\d{2}\\b', '', date_str)     # 09:00\n",
        "    # 4ìë¦¬ ì‹œê°„: ê³µë°±/í‹ˆ ë’¤ & ê³µë°±/ë¬¸ìì—´ë ì•ì— ìœ„ì¹˜í•  ë•Œë§Œ ì œê±°\n",
        "    date_str = re.sub(r'(?<=\\s)(?:[01]\\d[0-5]\\d|2[0-3][0-5]\\d)(?=\\s|$)', '', date_str)\n",
        "    date_str = date_str.replace('-', '.').replace('/', '.')\n",
        "    date_str = re.sub(r'\\s+', ' ', date_str).strip()\n",
        "\n",
        "    # â”€â”€ (2) YYYY.MM.DD ~ YYYY.MM.DD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    m = re.search(\n",
        "        r'(\\d{4})\\.(\\d{1,2})\\.(\\d{1,2})\\s*~\\s*'\n",
        "        r'(\\d{4})\\.(\\d{1,2})\\.(\\d{1,2})', date_str)\n",
        "    if m:\n",
        "        y1, m1, d1, y2, m2, d2 = map(int, m.groups())\n",
        "        return f\"{datetime(y1, m1, d1):%Y.%m.%d} ~ {datetime(y2, m2, d2):%Y.%m.%d}\"\n",
        "\n",
        "    # â”€â”€ (3) YYYY.MM.DD ~ MM.DD  (ë’¤ ì—°ë„ ìƒëµ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    m = re.search(\n",
        "        r'(\\d{4})\\.(\\d{1,2})\\.(\\d{1,2})\\s*~\\s*'\n",
        "        r'(\\d{1,2})\\.(\\d{1,2})', date_str)\n",
        "    if m:\n",
        "        y, m1, d1, m2, d2 = map(int, m.groups())\n",
        "        start = datetime(y,  m1, d1)\n",
        "        end   = datetime(y,  m2, d2)\n",
        "        if end < start:                       # ì—°ë§ ë„˜ê¸°ë©´ +1y\n",
        "            end = datetime(y+1, m2, d2)\n",
        "        return f\"{start:%Y.%m.%d} ~ {end:%Y.%m.%d}\"\n",
        "\n",
        "    # â”€â”€ (4) YYYY.MM.DD â€¦ ~ HH:MM (ê°™ì€ ë‚ ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    m = re.search(r'(\\d{4})\\.(\\d{1,2})\\.(\\d{1,2})\\s*~', date_str)\n",
        "    if m:\n",
        "        y, mth, d = map(int, m.groups())\n",
        "        dt = datetime(y, mth, d)\n",
        "        return f\"{dt:%Y.%m.%d} ~ {dt:%Y.%m.%d}\"\n",
        "\n",
        "    # â”€â”€ (5) ë‹¨ì¼ ë‚ ì§œ (YYYY.MM.DD) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    m = re.match(r'(\\d{4})\\.(\\d{1,2})\\.(\\d{1,2})$', date_str)\n",
        "    if m:\n",
        "        y, mth, d = map(int, m.groups())\n",
        "        dt = datetime(y, mth, d)\n",
        "        return f\"{dt:%Y.%m.%d} ~ {dt:%Y.%m.%d}\"\n",
        "\n",
        "    # ì´ì™¸ íŒ¨í„´ì€ ì›ë³¸ ìœ ì§€\n",
        "    return date_str\n",
        "\n",
        "# date ì»¬ëŸ¼ì„ 'YYYY.MM.DD ~ YYYY.MM.DD' í˜•ì‹ìœ¼ë¡œ í†µì¼\n",
        "df[\"date\"] = df[\"date\"].apply(normalize_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [07:45<00:00,  6.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ì €ì¥ ì™„ë£Œ â†’ data_with_desc_dev_category.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# LLMì„ ì´ìš©í•œ ë°ì´í„° ì „ì²˜ë¦¬ \n",
        "import os, time, textwrap, json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ë° OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "def get_description(url: str, *, retry: int = 3) -> str:\n",
        "    \"\"\"ì›¹í”„ë¦¬ë·°ë¡œ URL í•µì‹¬ ë‚´ìš©(ì„¤ëª…) ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
        "    prompt = f\"{url}\\n\\nì´ í˜ì´ì§€ì˜ ì£¼ìš” ì •ë³´ë¥¼ 7ì¤„ ì´ë‚´ì˜ ì¤„ê¸€ í˜•ì‹ìœ¼ë¡œ ë¬¸ë‹¨í˜• ìš”ì•½ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.\"\n",
        "    for _ in range(retry):\n",
        "        try:\n",
        "            res = client.responses.create(\n",
        "                model=\"gpt-4.1\",\n",
        "                tools=[{\"type\": \"web_search_preview\"}],\n",
        "                input=prompt,\n",
        "            )\n",
        "            return res.output_text.strip()\n",
        "        except Exception as e:\n",
        "            print(\"âš ï¸ desc ì¬ì‹œë„:\", e)\n",
        "            time.sleep(2)\n",
        "    return \"\"\n",
        "\n",
        "def classify(text: str) -> tuple[bool, str]:\n",
        "    \"\"\"ì œëª©+ì„¤ëª… í…ìŠ¤íŠ¸ â†’ (is_dev_event, category)\"\"\"\n",
        "    system = \"ë‹¹ì‹ ì€ í–‰ì‚¬ ì •ë³´ë¥¼ ë¶„ì„í•´ JSONìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” AIì…ë‹ˆë‹¤.\"\n",
        "    user = f\"\"\"\n",
        "â”€â”€â”€â”€ í–‰ì‚¬ ì •ë³´ â”€â”€â”€â”€\n",
        "{text}\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "1) ê°œë°œìÂ·ê¸°ìˆ  í–‰ì‚¬ë©´ \"yes\", ì•„ë‹ˆë©´ \"no\"\n",
        "2) ì¹´í…Œê³ ë¦¬ëŠ” í•˜ë‚˜ ì„ íƒ:\n",
        "   ê³µëª¨ì „/ëŒ€íšŒ | ë¶€íŠ¸ìº í”„/êµìœ¡ | ì»¨í¼ëŸ°ìŠ¤/í¬ëŸ¼ | ë°‹ì—…/ë„¤íŠ¸ì›Œí‚¹ | ê¸°íƒ€\n",
        "JSONë§Œ:\n",
        "{{\"is_dev_event\":\"yes\",\"category\":\"ë¶€íŠ¸ìº í”„/êµìœ¡\"}}\n",
        "\"\"\"\n",
        "    res = client.chat.completions.create(\n",
        "        model=\"gpt-4.1\",\n",
        "        messages=[{\"role\": \"system\", \"content\": system},\n",
        "                  {\"role\": \"user\",   \"content\": textwrap.dedent(user)}],\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=0,\n",
        "        max_tokens=120,\n",
        "    )\n",
        "    try:\n",
        "        data = json.loads(res.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
        "        return False, \"ê¸°íƒ€\"\n",
        "\n",
        "    is_dev = str(data.get(\"is_dev_event\", \"\")).lower().startswith(\"y\")\n",
        "    cat    = data.get(\"category\", \"ê¸°íƒ€\")\n",
        "    if cat not in ['ê³µëª¨ì „/ëŒ€íšŒ','ë¶€íŠ¸ìº í”„/êµìœ¡','ì»¨í¼ëŸ°ìŠ¤/í¬ëŸ¼','ë°‹ì—…/ë„¤íŠ¸ì›Œí‚¹','ê¸°íƒ€']:\n",
        "        cat = \"ê¸°íƒ€\"\n",
        "    return is_dev, cat\n",
        "\n",
        "# â”€â”€â”€ ì‹¤í–‰ ì…€ (DataFrame ì²˜ë¦¬) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "tqdm.pandas()\n",
        "\n",
        "def pipeline(row):\n",
        "    desc = get_description(row[\"url\"])\n",
        "    merged = f\"ì œëª©: {row['title']}\\nì„¤ëª…: {desc}\"\n",
        "    is_dev, cat = classify(merged)\n",
        "    return pd.Series({\"description\": desc,\n",
        "                      \"is_dev_event\": is_dev,\n",
        "                      \"category\": cat})\n",
        "\n",
        "df[[\"description\", \"is_dev_event\", \"category\"]] = (\n",
        "    df.progress_apply(pipeline, axis=1)\n",
        ")\n",
        "\n",
        "df[[\"title\", \"description\", \"is_dev_event\", \"category\"]].to_csv(\"data_with_desc_dev_category.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "df.to_csv(\"data_with_new_features.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"âœ… ì €ì¥ ì™„ë£Œ â†’ data_with_new_features.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>host</th>\n",
              "      <th>date</th>\n",
              "      <th>url</th>\n",
              "      <th>description</th>\n",
              "      <th>is_dev_event</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [title, host, date, url, description, is_dev_event, category]\n",
              "Index: []"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ì €ì¥ ì™„ë£Œ â†’ preprocessed_data.csv\n"
          ]
        }
      ],
      "source": [
        "# ê°œë°œ í–‰ì‚¬ ë¶„ë¥˜\n",
        "# TRUEì¸ í–‰ë§Œ í•„í„°ë§\n",
        "df = df[df[\"is_dev_event\"] == \"TRUE\"].copy()\n",
        "\n",
        "# ì»¬ëŸ¼ ì‚­ì œ\n",
        "df.drop(columns=[\"is_dev_event\"], inplace=True)\n",
        "\n",
        "# ì €ì¥\n",
        "df.to_csv(\"preprocessed_data.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"âœ… ì €ì¥ ì™„ë£Œ â†’ preprocessed_data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
            "  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
            "Collecting torch>=1.11.0 (from sentence-transformers)\n",
            "  Using cached torch-2.7.1-cp310-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from sentence-transformers) (1.7.0)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
            "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.33.2-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting Pillow (from sentence-transformers)\n",
            "  Downloading pillow-11.3.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from sentence-transformers) (4.13.2)\n",
            "Collecting filelock (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
            "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Downloading tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
            "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
            "  Downloading hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
            "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.6.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
            "Downloading transformers-4.53.1-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n",
            "Downloading hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
            "Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
            "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
            "Using cached torch-2.7.1-cp310-none-macosx_11_0_arm64.whl (68.6 MB)\n",
            "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "Downloading pillow-11.3.0-cp310-cp310-macosx_11_0_arm64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, sympy, safetensors, regex, Pillow, networkx, hf-xet, fsspec, filelock, torch, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14/14\u001b[0m [sentence-transformers]ence-transformers]\n",
            "\u001b[1A\u001b[2KSuccessfully installed Pillow-11.3.0 filelock-3.18.0 fsspec-2025.5.1 hf-xet-1.1.5 huggingface-hub-0.33.2 mpmath-1.3.0 networkx-3.4.2 regex-2024.11.6 safetensors-0.5.3 sentence-transformers-5.0.0 sympy-1.14.0 tokenizers-0.21.2 torch-2.7.1 transformers-4.53.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë¬¸ì¥ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# 1. ëª¨ë¸ ë¡œë“œ (ìµœì´ˆ 1íšŒë§Œ í•„ìš”)\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74bef9cd7b164ef88c63af717efa8fa6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d859e7cee0fd4d1b9a1010a295ef325c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e04f07b0ad8a443f8a86ec89c8fb9007",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b4746e17669474e85e9cc142d901ae2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf66d3e8d4204d36b3f056978557fde5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b0aac9258524932baa6869b16f96d99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1987d6bd5d4d4020995e9fbb928e270f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "554fff6ef0d348208fc715d645461890",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2de069708574b3e9871bf5c2a231628",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71d5e652ed0e4025a494bddfbdf0bf62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” ì‚¬ìš©ì ê´€ì‹¬ ë¬¸ì¥: ì´ í–‰ì‚¬ëŠ” ë°±ì—”ë“œ, í´ë¼ìš°ë“œ, AI, LLM ê¸°ìˆ ì— ê´€ì‹¬ ìˆëŠ” ê°œë°œìë¥¼ ìœ„í•œ í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.\n",
            "ğŸ“ ì´ë²¤íŠ¸ ì„¤ëª… ìš”ì•½: AWS TechCampëŠ” AWS í´ë¼ìš°ë“œ ê¸°ì´ˆë¶€í„° ì‹¬í™”ê¹Œì§€ ë°°ìš°ëŠ” 3ì¼ ê³¼ì •ì˜ ë¬´ë£Œ êµìœ¡ í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.\n",
            "ì´ë¡ ê³¼ ì‹¤ìŠµì„ ë³‘í–‰í•˜ë©°, ì˜¨ë¼ì¸ê³¼ ì˜¤í”„ë¼ì¸ìœ¼ë¡œ ìš´ì˜ë©ë‹ˆë‹¤.\n",
            "ì„¸ì…˜ì€ ìˆ˜ì¤€ë³„Â·ì‚°ì—…ë³„ë¡œ ë‹¤ì–‘í•˜ê²Œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
            "í´ë¼ìš°ë“œ ì…ë¬¸ìë¶€í„° ì‹¤ë¬´ìê¹Œì§€ ëª¨ë‘ ì°¸ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "ğŸ“Š ìœ ì‚¬ë„ ì ìˆ˜: 0.6317\n"
          ]
        }
      ],
      "source": [
        "# 2. ì‚¬ìš©ì ê´€ì‹¬ í‚¤ì›Œë“œ â†’ ë¬¸ì¥í˜•ìœ¼ë¡œ ë³€í™˜\n",
        "user_keywords = [\"ë°±ì—”ë“œ\", \"í´ë¼ìš°ë“œ\", \"AI\", \"LLM\"]\n",
        "user_sentence = f\"ì´ í–‰ì‚¬ëŠ” {', '.join(user_keywords)} ê¸°ìˆ ì— ê´€ì‹¬ ìˆëŠ” ê°œë°œìë¥¼ ìœ„í•œ í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.\"\n",
        "\n",
        "# 3. ë¹„êµ ëŒ€ìƒ ì„¤ëª… (ì˜ˆ: ì´ë²¤íŠ¸ ì„¤ëª… ìš”ì•½)\n",
        "event_description = \"\"\"\n",
        "AWS TechCampëŠ” AWS í´ë¼ìš°ë“œ ê¸°ì´ˆë¶€í„° ì‹¬í™”ê¹Œì§€ ë°°ìš°ëŠ” 3ì¼ ê³¼ì •ì˜ ë¬´ë£Œ êµìœ¡ í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.\n",
        "ì´ë¡ ê³¼ ì‹¤ìŠµì„ ë³‘í–‰í•˜ë©°, ì˜¨ë¼ì¸ê³¼ ì˜¤í”„ë¼ì¸ìœ¼ë¡œ ìš´ì˜ë©ë‹ˆë‹¤.\n",
        "ì„¸ì…˜ì€ ìˆ˜ì¤€ë³„Â·ì‚°ì—…ë³„ë¡œ ë‹¤ì–‘í•˜ê²Œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "í´ë¼ìš°ë“œ ì…ë¬¸ìë¶€í„° ì‹¤ë¬´ìê¹Œì§€ ëª¨ë‘ ì°¸ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\"\"\"\n",
        "\n",
        "# 4. ë²¡í„° ì„ë² ë”©\n",
        "emb_user = model.encode(user_sentence, convert_to_tensor=True)\n",
        "emb_event = model.encode(event_description, convert_to_tensor=True)\n",
        "\n",
        "# 5. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "similarity = util.cos_sim(emb_user, emb_event).item()\n",
        "\n",
        "# 6. ê²°ê³¼ ì¶œë ¥\n",
        "print(f\"ğŸ“Š ìœ ì‚¬ë„ ì ìˆ˜: {similarity:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dev-event",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
