{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 크롤링 모듈 import\n",
        "from wevity_crawler import crawl_wevity\n",
        "from dacon_crawler import crawl_dacon\n",
        "from devEvent_crawler import crawl_devEvent\n",
        "from inflearn_crawler import crawl_inflearn\n",
        "\n",
        "# 각 사이트에서 데이터 크롤링\n",
        "df_wevity = crawl_wevity()\n",
        "df_dev = crawl_devEvent()\n",
        "df_inflearn = crawl_inflearn()\n",
        "df_dacon = crawl_dacon(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 데이터 통합\n",
        "df = pd.concat([df_wevity, df_dacon, df_dev, df_inflearn], ignore_index=True)\n",
        "print(f\"총 {len(df)}개 이벤트 통합됨\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 정규표현식을 이용한 데이터 전처리\n",
        "import re\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "def normalize_date(date_str: str) -> str:\n",
        "    \"\"\"\n",
        "    날짜 문자열을 'YYYY.MM.DD ~ YYYY.MM.DD' 형식으로 통일한다.\n",
        "\n",
        "    규칙\n",
        "    ───────────────────────────────────────────────\n",
        "    1. 괄호 안 정보·시간(HH:MM, HHMM)·불필요 공백 제거\n",
        "    2. 'YYYY.MM.DD ~ YYYY.MM.DD'  → 그대로 포맷\n",
        "    3. 'YYYY.MM.DD ~ MM.DD'       → 뒤쪽 연도 = 앞쪽 연도\n",
        "    4. 'YYYY.MM.DD … ~ HH:MM'     → 같은 날 범위\n",
        "    5. 단일 날짜(YYYY.MM.DD)      → 앞뒤 날짜 동일\n",
        "    \"\"\"\n",
        "    date_str = str(date_str).strip()\n",
        "\n",
        "    # ── (1) 노이즈 제거 ──────────────────────────────────\n",
        "    date_str = re.sub(r'\\(.*?\\)', '', date_str)               # (요일) (한국 표준시)\n",
        "    date_str = re.sub(r'\\b\\d{1,2}:\\d{2}\\b', '', date_str)     # 09:00\n",
        "    # 4자리 시간: 공백/틈 뒤 & 공백/문자열끝 앞에 위치할 때만 제거\n",
        "    date_str = re.sub(r'(?<=\\s)(?:[01]\\d[0-5]\\d|2[0-3][0-5]\\d)(?=\\s|$)', '', date_str)\n",
        "    date_str = date_str.replace('-', '.').replace('/', '.')\n",
        "    date_str = re.sub(r'\\s+', ' ', date_str).strip()\n",
        "\n",
        "    # ── (2) YYYY.MM.DD ~ YYYY.MM.DD ─────────────────────\n",
        "    m = re.search(\n",
        "        r'(\\d{4})\\.(\\d{1,2})\\.(\\d{1,2})\\s*~\\s*'\n",
        "        r'(\\d{4})\\.(\\d{1,2})\\.(\\d{1,2})', date_str)\n",
        "    if m:\n",
        "        y1, m1, d1, y2, m2, d2 = map(int, m.groups())\n",
        "        return f\"{datetime(y1, m1, d1):%Y.%m.%d} ~ {datetime(y2, m2, d2):%Y.%m.%d}\"\n",
        "\n",
        "    # ── (3) YYYY.MM.DD ~ MM.DD  (뒤 연도 생략) ───────────\n",
        "    m = re.search(\n",
        "        r'(\\d{4})\\.(\\d{1,2})\\.(\\d{1,2})\\s*~\\s*'\n",
        "        r'(\\d{1,2})\\.(\\d{1,2})', date_str)\n",
        "    if m:\n",
        "        y, m1, d1, m2, d2 = map(int, m.groups())\n",
        "        start = datetime(y,  m1, d1)\n",
        "        end   = datetime(y,  m2, d2)\n",
        "        if end < start:                       # 연말 넘기면 +1y\n",
        "            end = datetime(y+1, m2, d2)\n",
        "        return f\"{start:%Y.%m.%d} ~ {end:%Y.%m.%d}\"\n",
        "\n",
        "    # ── (4) YYYY.MM.DD … ~ HH:MM (같은 날) ──────────────\n",
        "    m = re.search(r'(\\d{4})\\.(\\d{1,2})\\.(\\d{1,2})\\s*~', date_str)\n",
        "    if m:\n",
        "        y, mth, d = map(int, m.groups())\n",
        "        dt = datetime(y, mth, d)\n",
        "        return f\"{dt:%Y.%m.%d} ~ {dt:%Y.%m.%d}\"\n",
        "\n",
        "    # ── (5) 단일 날짜 (YYYY.MM.DD) ──────────────────────\n",
        "    m = re.match(r'(\\d{4})\\.(\\d{1,2})\\.(\\d{1,2})$', date_str)\n",
        "    if m:\n",
        "        y, mth, d = map(int, m.groups())\n",
        "        dt = datetime(y, mth, d)\n",
        "        return f\"{dt:%Y.%m.%d} ~ {dt:%Y.%m.%d}\"\n",
        "\n",
        "    # 이외 패턴은 원본 유지\n",
        "    return date_str\n",
        "\n",
        "# date 컬럼을 'YYYY.MM.DD ~ YYYY.MM.DD' 형식으로 통일\n",
        "df[\"date\"] = df[\"date\"].apply(normalize_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 75/75 [07:45<00:00,  6.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 저장 완료 → data_with_desc_dev_category.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# LLM을 이용한 데이터 전처리 \n",
        "import os, time, textwrap, json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# 환경변수 로드 및 OpenAI 클라이언트 설정\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "def get_description(url: str, *, retry: int = 3) -> str:\n",
        "    \"\"\"웹프리뷰로 URL 핵심 내용(설명) 가져오기\"\"\"\n",
        "    prompt = f\"{url}\\n\\n이 페이지의 주요 정보를 7줄 이내의 줄글 형식으로 문단형 요약을 작성해 주세요.\"\n",
        "    for _ in range(retry):\n",
        "        try:\n",
        "            res = client.responses.create(\n",
        "                model=\"gpt-4.1\",\n",
        "                tools=[{\"type\": \"web_search_preview\"}],\n",
        "                input=prompt,\n",
        "            )\n",
        "            return res.output_text.strip()\n",
        "        except Exception as e:\n",
        "            print(\"⚠️ desc 재시도:\", e)\n",
        "            time.sleep(2)\n",
        "    return \"\"\n",
        "\n",
        "def classify(text: str) -> tuple[bool, str]:\n",
        "    \"\"\"제목+설명 텍스트 → (is_dev_event, category)\"\"\"\n",
        "    system = \"당신은 행사 정보를 분석해 JSON으로 반환하는 AI입니다.\"\n",
        "    user = f\"\"\"\n",
        "──── 행사 정보 ────\n",
        "{text}\n",
        "──────────────────\n",
        "1) 개발자·기술 행사면 \"yes\", 아니면 \"no\"\n",
        "2) 카테고리는 하나 선택:\n",
        "   공모전/대회 | 부트캠프/교육 | 컨퍼런스/포럼 | 밋업/네트워킹 | 기타\n",
        "JSON만:\n",
        "{{\"is_dev_event\":\"yes\",\"category\":\"부트캠프/교육\"}}\n",
        "\"\"\"\n",
        "    res = client.chat.completions.create(\n",
        "        model=\"gpt-4.1\",\n",
        "        messages=[{\"role\": \"system\", \"content\": system},\n",
        "                  {\"role\": \"user\",   \"content\": textwrap.dedent(user)}],\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=0,\n",
        "        max_tokens=120,\n",
        "    )\n",
        "    try:\n",
        "        data = json.loads(res.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ JSON 파싱 실패:\", e)\n",
        "        return False, \"기타\"\n",
        "\n",
        "    is_dev = str(data.get(\"is_dev_event\", \"\")).lower().startswith(\"y\")\n",
        "    cat    = data.get(\"category\", \"기타\")\n",
        "    if cat not in ['공모전/대회','부트캠프/교육','컨퍼런스/포럼','밋업/네트워킹','기타']:\n",
        "        cat = \"기타\"\n",
        "    return is_dev, cat\n",
        "\n",
        "# ─── 실행 셀 (DataFrame 처리) ────────────────────────────\n",
        "tqdm.pandas()\n",
        "\n",
        "def pipeline(row):\n",
        "    desc = get_description(row[\"url\"])\n",
        "    merged = f\"제목: {row['title']}\\n설명: {desc}\"\n",
        "    is_dev, cat = classify(merged)\n",
        "    return pd.Series({\"description\": desc,\n",
        "                      \"is_dev_event\": is_dev,\n",
        "                      \"category\": cat})\n",
        "\n",
        "df[[\"description\", \"is_dev_event\", \"category\"]] = (\n",
        "    df.progress_apply(pipeline, axis=1)\n",
        ")\n",
        "\n",
        "df[[\"title\", \"description\", \"is_dev_event\", \"category\"]].to_csv(\"data_with_desc_dev_category.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "df.to_csv(\"data_with_new_features.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"✅ 저장 완료 → data_with_new_features.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>host</th>\n",
              "      <th>date</th>\n",
              "      <th>url</th>\n",
              "      <th>description</th>\n",
              "      <th>is_dev_event</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [title, host, date, url, description, is_dev_event, category]\n",
              "Index: []"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 저장 완료 → preprocessed_data.csv\n"
          ]
        }
      ],
      "source": [
        "# 개발 행사 분류\n",
        "# TRUE인 행만 필터링\n",
        "df = df[df[\"is_dev_event\"] == \"TRUE\"].copy()\n",
        "\n",
        "# 컬럼 삭제\n",
        "df.drop(columns=[\"is_dev_event\"], inplace=True)\n",
        "\n",
        "# 저장\n",
        "df.to_csv(\"preprocessed_data.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"✅ 저장 완료 → preprocessed_data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
            "  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
            "Collecting torch>=1.11.0 (from sentence-transformers)\n",
            "  Using cached torch-2.7.1-cp310-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from sentence-transformers) (1.7.0)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
            "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.33.2-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting Pillow (from sentence-transformers)\n",
            "  Downloading pillow-11.3.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from sentence-transformers) (4.13.2)\n",
            "Collecting filelock (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
            "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Downloading tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
            "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
            "  Downloading hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
            "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.6.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/dev-event/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
            "Downloading transformers-4.53.1-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n",
            "Downloading hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
            "Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
            "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
            "Using cached torch-2.7.1-cp310-none-macosx_11_0_arm64.whl (68.6 MB)\n",
            "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "Downloading pillow-11.3.0-cp310-cp310-macosx_11_0_arm64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, sympy, safetensors, regex, Pillow, networkx, hf-xet, fsspec, filelock, torch, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [sentence-transformers]ence-transformers]\n",
            "\u001b[1A\u001b[2KSuccessfully installed Pillow-11.3.0 filelock-3.18.0 fsspec-2025.5.1 hf-xet-1.1.5 huggingface-hub-0.33.2 mpmath-1.3.0 networkx-3.4.2 regex-2024.11.6 safetensors-0.5.3 sentence-transformers-5.0.0 sympy-1.14.0 tokenizers-0.21.2 torch-2.7.1 transformers-4.53.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 문장 유사도 계산\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# 1. 모델 로드 (최초 1회만 필요)\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74bef9cd7b164ef88c63af717efa8fa6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d859e7cee0fd4d1b9a1010a295ef325c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e04f07b0ad8a443f8a86ec89c8fb9007",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b4746e17669474e85e9cc142d901ae2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf66d3e8d4204d36b3f056978557fde5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b0aac9258524932baa6869b16f96d99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1987d6bd5d4d4020995e9fbb928e270f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "554fff6ef0d348208fc715d645461890",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2de069708574b3e9871bf5c2a231628",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71d5e652ed0e4025a494bddfbdf0bf62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 사용자 관심 문장: 이 행사는 백엔드, 클라우드, AI, LLM 기술에 관심 있는 개발자를 위한 프로그램입니다.\n",
            "📝 이벤트 설명 요약: AWS TechCamp는 AWS 클라우드 기초부터 심화까지 배우는 3일 과정의 무료 교육 프로그램입니다.\n",
            "이론과 실습을 병행하며, 온라인과 오프라인으로 운영됩니다.\n",
            "세션은 수준별·산업별로 다양하게 구성되어 있습니다.\n",
            "클라우드 입문자부터 실무자까지 모두 참여할 수 있습니다.\n",
            "📊 유사도 점수: 0.6317\n"
          ]
        }
      ],
      "source": [
        "# 2. 사용자 관심 키워드 → 문장형으로 변환\n",
        "user_keywords = [\"백엔드\", \"클라우드\", \"AI\", \"LLM\"]\n",
        "user_sentence = f\"이 행사는 {', '.join(user_keywords)} 기술에 관심 있는 개발자를 위한 프로그램입니다.\"\n",
        "\n",
        "# 3. 비교 대상 설명 (예: 이벤트 설명 요약)\n",
        "event_description = \"\"\"\n",
        "AWS TechCamp는 AWS 클라우드 기초부터 심화까지 배우는 3일 과정의 무료 교육 프로그램입니다.\n",
        "이론과 실습을 병행하며, 온라인과 오프라인으로 운영됩니다.\n",
        "세션은 수준별·산업별로 다양하게 구성되어 있습니다.\n",
        "클라우드 입문자부터 실무자까지 모두 참여할 수 있습니다.\n",
        "\"\"\"\n",
        "\n",
        "# 4. 벡터 임베딩\n",
        "emb_user = model.encode(user_sentence, convert_to_tensor=True)\n",
        "emb_event = model.encode(event_description, convert_to_tensor=True)\n",
        "\n",
        "# 5. 코사인 유사도 계산\n",
        "similarity = util.cos_sim(emb_user, emb_event).item()\n",
        "\n",
        "# 6. 결과 출력\n",
        "print(f\"📊 유사도 점수: {similarity:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dev-event",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
