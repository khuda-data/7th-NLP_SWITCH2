import re
from bs4 import BeautifulSoup
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from urllib.parse import urljoin
import time
import pandas as pd
import os
import html

def extract_organizer(soup):
    """ì£¼ìµœì ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    for tag in soup.select("h3, strong, b"):
        if "ì£¼ìµœ" in tag.get_text():
            for sibling in tag.find_all_next():
                if sibling.name in ["h3", "strong", "b"] and sibling != tag:
                    break
                if sibling.name == "p":
                    text = sibling.get_text(strip=True)
                    if text:
                        return text
                if sibling.name == "ul":
                    items = [li.get_text(strip=True) for li in sibling.find_all("li") if li.get_text(strip=True)]
                    if items:
                        return " / ".join(items)
    return "ì£¼ìµœ ì •ë³´ ì—†ìŒ"


def clean_organizer(raw_text):
    """ì£¼ìµœì ì •ë³´ë¥¼ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    raw_text = raw_text.strip().replace("ï¼š", ":")
    matches = re.findall(r"(ì£¼ìµœ[^:ï¼š]*[:ï¼š]\s*[^/]+)", raw_text)
    if not matches:
        return raw_text.strip() if raw_text else "ì£¼ìµœ ì •ë³´ ì—†ìŒ"

    orgs = []
    for match in matches:
        _, org_text = match.split(":", 1)
        orgs.extend([o.strip() for o in org_text.split(",") if o.strip()])
    return ", ".join(orgs) if orgs else "ì£¼ìµœ ì •ë³´ ì—†ìŒ"


def extract_date_range_string(soup: BeautifulSoup) -> str:
    li_tags = soup.find_all("li", class_="text-body-2")
    for li in li_tags:
        text = li.get_text(strip=True)
        # ë‚ ì§œ íŒ¨í„´ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•„ì„œ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
        match = re.search(r"\d{4}\.\d{2}\.\d{2}.*?(?:\d{4}\.\d{2}\.\d{2}|\d{2}:\d{2})", text)
        if match:
            return match.group().strip()
    return "ë‚ ì§œ ì •ë³´ ì—†ìŒ"


def crawl_dacon(limit=None):
    """
    ë°ì´ì½˜ ëŒ€íšŒ ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        limit (int, optional): í¬ë¡¤ë§í•  ëŒ€íšŒ ìˆ˜ ì œí•œ. Noneì´ë©´ ëª¨ë“  ëŒ€íšŒ
    
    Returns:
        pandas.DataFrame: í¬ë¡¤ë§ëœ ëŒ€íšŒ ì •ë³´
    """
    # ì²« ë²ˆì§¸ ë‹¨ê³„: ëª¨ë“  ëŒ€íšŒ URL ìˆ˜ì§‘
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.get("https://dacon.io/competitions")
    time.sleep(2)

    # ìˆ˜ì§‘ ì‹œì‘ í›„ ë”ë³´ê¸° í´ë¦­
    while True:
        try:
            load_more = driver.find_element(By.CSS_SELECTOR, "#main > button")
            driver.execute_script("arguments[0].scrollIntoView(true);", load_more)
            time.sleep(0.5)
            load_more.click()
            time.sleep(1.2)

            # ìŠ¤í¬ë¡¤ ë‹¤ìš´ ê°•ì œ â†’ ë” ë§ì€ ì¹´ë“œ ë Œë”ë§ ìœ ë„
            for _ in range(3):
                driver.execute_script("window.scrollBy(0, 1000);")
                time.sleep(0.3)

        except:
            break

    # comp div ìˆœíšŒ
    comp_divs = driver.find_elements(By.CSS_SELECTOR, "div.comp")
    urls = []
    for div in comp_divs:
        try:
            a_tag = div.find_element(By.TAG_NAME, "a")
            href = a_tag.get_attribute("href")
            if href:
                urls.append(urljoin("https://dacon.io", href))
        except:
            continue

    driver.quit()

    # ì œí•œì´ ìˆë‹¤ë©´ ì ìš©
    if limit:
        urls = urls[:limit]

    # ë‘ ë²ˆì§¸ ë‹¨ê³„: ê° ëŒ€íšŒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    results = []
    
    # ìƒˆë¡œìš´ ë“œë¼ì´ë²„ ìƒì„±
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    for url in urls:
        try:
            driver.get(url)
            time.sleep(3)  # JS ë¡œë”© ì‹œê°„ ê¸°ë‹¤ë¦¬ê¸°

            soup = BeautifulSoup(driver.page_source, "html.parser")

            title_elem = soup.select_one("h1")
            title = title_elem.text.strip() if title_elem else "ì œëª© ì—†ìŒ"

            status_elem = soup.select_one("li.d-inline.text-body-2 span.ms-5")
            status = status_elem.text.strip() if status_elem else ""

            if "ë§ˆê°" in status:
                print(f"â›” ë§ˆê°ëœ ëŒ€íšŒ ìŠ¤í‚µ: {title}")
                continue

            organizer_raw = extract_organizer(soup)
            organizer_cleaned = clean_organizer(organizer_raw)
            date = extract_date_range_string(soup)

            div = soup.select_one("div.CompetitionDetailTitleSection")
            img_url = None
            if div:
                style = div.get("style", "")
                print(f"[DEBUG] style: {style}")  # í™•ì¸ìš©

                style = html.unescape(style)
                m = re.search(r'background-image\s*:\s*url\((["\']?)(.*?)\1\)', style)
                if m:
                    img_url = m.group(2)

            results.append({
                "title": title,
                "date": date,
                "host": organizer_cleaned,
                "url": url,
                "img_url": img_url
            })

            print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {title}")

        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ: {url}\n   â†³ {type(e).__name__}: {e}")

    # í¬ë¡¬ ë“œë¼ì´ë²„ ì¢…ë£Œ
    driver.quit()

    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(results)
    return df


if __name__ == "__main__":
    """ìŠ¤í¬ë¦½íŠ¸ ë‹¨ë… ì‹¤í–‰ ì‹œ: í¬ë¡¤ë§ â†’ CSV ì €ì¥."""
    print("ğŸš€ Starting Dacon crawling...")
    
    # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 10ê°œë§Œ ìˆ˜ì§‘
    df = crawl_dacon(limit=10)
    
    if df.empty:
        print("\nâš ï¸  ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        os.makedirs("output", exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        out_path = os.path.join("output", f"dacon_competitions_{timestamp}.csv")
        df.to_csv(out_path, index=False, encoding="utf-8-sig")
        print(f"\nğŸ‰ {len(df):,}ê±´ ì €ì¥ ì™„ë£Œ â†’ {out_path}") 