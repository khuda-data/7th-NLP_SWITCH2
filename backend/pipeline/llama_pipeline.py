# pipeline/rag_pipeline.py
from retriever.retriever import retrieve
from generator.llama import generate_llama_answer
from generator.llama import llm  # llm ê°ì²´ë¥¼ ì§ì ‘ ì‚¬ìš©

def llama_pipeline(user_question):
    related_docs = retrieve(user_question)

    def select_top_docs_by_token_limit(docs, max_tokens=400):
        selected = []
        total = 0
        for text in docs:  # âœ… ë¬¸ìì—´ ë°”ë¡œ ì‚¬ìš©
            token_len = len(llm.tokenize(text.encode("utf-8")))
            if total + token_len > max_tokens:
                break
            selected.append(text)
            total += token_len
        return selected

    selected_context = select_top_docs_by_token_limit(related_docs)
    print("ğŸ” Selected context:\n", selected_context)

    if not selected_context:
        return "í•´ë‹¹ ì£¼ì œì˜ ê³µëª¨ì „ì€ í˜„ì¬ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

    answer = generate_llama_answer(user_question, selected_context)
    print("ğŸ’¬ Generated answer:\n", answer)
    return answer