from playwright.async_api import async_playwright
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup, NavigableString
from urllib.parse import urljoin
import pandas as pd
import re
import time
from datetime import datetime
import asyncio
import os

# ---------------------------------------------------------------------------
# í•µì‹¬ íŒŒë¼ë¯¸í„° (í•„ìš”ì‹œ ì•„ë˜ ê°’ë§Œ ìˆ˜ì •í•´ ì‚¬ìš©)
# ---------------------------------------------------------------------------
SECTION_HINTS = ("[ë°°ê²½]", "[ì£¼ì œ]", "[ì„¤ëª…]", "[ì°¸ê°€", "[ì£¼ìµœ", "[ì‹œìƒ")
TERMS_PATTERN = re.compile(r"ì œ\s*\d+\s*ì¡°")      # ì•½ê´€ ì¡°ë¬¸ ì‹ë³„ìš©

# ---------------------------------------------------------------------------
# 1) ìƒì„¸ í˜ì´ì§€ HTML ì¤‘ ql-editor ë¸”ë¡ ì¶”ì¶œ & ì •ì œ
# ---------------------------------------------------------------------------
async def fetch_clean_body(url: str) -> str:
    """ë°ì´ì½˜ ìƒì„¸ í˜ì´ì§€ì—ì„œ *ì†Œê°œ ì˜ì—­*ìœ¼ë¡œ ë³´ì´ëŠ” div.ql-editor 1ê°œë¥¼ ê³¨ë¼ HTML ë°˜í™˜"""
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                            "AppleWebKit/537.36 (KHTML, like Gecko) "
                            "Chrome/122.0.0.0 Safari/537.36")
            )
            page = await context.new_page()
            await page.goto(url, wait_until="load", timeout=40000)
            await page.wait_for_selector("div.ql-editor", timeout=15000)

            # --- â‘  ëª¨ë“  ql-editor í›„ë³´ ìˆ˜ì§‘ ---
            nodes = await page.query_selector_all("div.ql-editor")
            html_candidates: list[str] = []
            for n in nodes:
                try:
                    html_candidates.append(await n.inner_html())
                except Exception:
                    continue
            await browser.close()

            if not html_candidates:
                return ""  # nothing found

            # --- â‘¡ í—¤ë” í‚¤ì›Œë“œ í¬í•¨ ë¸”ë¡ ìš°ì„  ì„ íƒ ---
            def contains_section(html: str) -> bool:
                text = BeautifulSoup(html, "html.parser").get_text(" ", strip=True)
                return any(h in text for h in SECTION_HINTS)

            primary = next((h for h in html_candidates if contains_section(h)), None)

            # --- â‘¢ fallback: ì•½ê´€(ì œ n ì¡°)Â·ê¸¸ì´ 2000ìâ†‘ ì œì™¸ í›„ ê°€ì¥ ì§§ì€ ë¸”ë¡ ---
            if primary is None:
                def looks_like_terms(html: str) -> bool:
                    text = BeautifulSoup(html, "html.parser").get_text(" ", strip=True)
                    return TERMS_PATTERN.search(text[:120]) is not None or len(text) > 2000
                filtered = [h for h in html_candidates if not looks_like_terms(h)]
                primary = min(filtered, key=len) if filtered else min(html_candidates, key=len)

            # --- â‘£ ë¶ˆí•„ìš” íƒœê·¸ ì •ë¦¬ ---
            soup = BeautifulSoup(f"<div class='ql-editor'>{primary}</div>", "html.parser")
            for tag in soup.find_all(['script', 'svg', 'path', 'iframe', 'noscript', 'br']):
                tag.decompose()
            return str(soup.div.prettify())

    except Exception as e:
        print(f"[HTML íŒŒì‹± ì‹¤íŒ¨] {url} - {e}")
        return ""

# ---------------------------------------------------------------------------
# 2) ì£¼ìµœìÂ·ë‚ ì§œ ë“± ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ë“¤
# ---------------------------------------------------------------------------

def extract_organizer(soup: BeautifulSoup) -> str:
    for tag in soup.select("h3, strong, b"):
        if "ì£¼ìµœ" in tag.get_text():
            for sibling in tag.find_all_next():
                if sibling.name in ["h3", "strong", "b"] and sibling != tag:
                    break
                if sibling.name == "p":
                    text = sibling.get_text(strip=True)
                    if text:
                        return text
                if sibling.name == "ul":
                    items = [li.get_text(strip=True) for li in sibling.find_all("li") if li.get_text(strip=True)]
                    if items:
                        return " / ".join(items)
    return "ì£¼ìµœ ì •ë³´ ì—†ìŒ"


def clean_organizer(raw_text: str) -> str:
    raw_text = raw_text.strip().replace("ï¼š", ":")
    matches = re.findall(r"(ì£¼ìµœ[^:ï¼š]*[:ï¼š]\s*[^/]+)", raw_text)
    if not matches:
        return raw_text.strip() if raw_text else "ì£¼ìµœ ì •ë³´ ì—†ìŒ"
    orgs = []
    for match in matches:
        _, org_text = match.split(":", 1)
        orgs.extend([o.strip() for o in org_text.split(",") if o.strip()])
    return ", ".join(orgs) if orgs else "ì£¼ìµœ ì •ë³´ ì—†ìŒ"


def extract_date_range_string(soup: BeautifulSoup) -> str:
    li_tags = soup.find_all("li", class_="text-body-2")
    for li in li_tags:
        text = li.get_text(strip=True)
        match = re.search(r"\d{4}\.\d{2}\.\d{2}\s*[~\-]\s*\d{4}\.\d{2}\.\d{2}(?:\s*\d{2}:\d{2})?", text)
        if match:
            raw = match.group()
            date_parts = re.findall(r'\d{4}\.\d{2}\.\d{2}', raw)
            if len(date_parts) == 2:
                start = datetime.strptime(date_parts[0], "%Y.%m.%d").strftime("%Y-%m-%d")
                end = datetime.strptime(date_parts[1], "%Y.%m.%d").strftime("%Y-%m-%d")
                return f"{start} ~ {end}"
            return raw
    return "ë‚ ì§œ ì •ë³´ ì—†ìŒ"

# ---------------------------------------------------------------------------
# 3) ë©”ì¸ í¬ë¡¤ë§ ë£¨í‹´
# ---------------------------------------------------------------------------
async def crawl_dacon(limit: int | None = None) -> pd.DataFrame:
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.get("https://dacon.io/competitions")
    time.sleep(2)

    # ëª¨ë“  ëŒ€íšŒ ë¡œë“œ ("ë”ë³´ê¸°" ë²„íŠ¼ ë°˜ë³µ í´ë¦­)
    while True:
        try:
            load_more = driver.find_element(By.CSS_SELECTOR, "#main > button")
            driver.execute_script("arguments[0].scrollIntoView(true);", load_more)
            time.sleep(0.4)
            load_more.click(); time.sleep(1.0)
            driver.execute_script("window.scrollBy(0, 1500);")
        except Exception:
            break

    urls = []
    for div in driver.find_elements(By.CSS_SELECTOR, "div.comp"):
        try:
            href = div.find_element(By.TAG_NAME, "a").get_attribute("href")
            if href:
                urls.append(urljoin("https://dacon.io", href))
        except Exception:
            continue
    driver.quit()

    # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    results = []
    for url in (urls if limit is None else urls[:limit]):
        try:
            driver.get(url); time.sleep(2.5)
            soup = BeautifulSoup(driver.page_source, "html.parser")

            title = soup.select_one("h1").get_text(strip=True) if soup.select_one("h1") else "ì œëª© ì—†ìŒ"
            status = soup.select_one("li.d-inline.text-body-2 span.ms-5")
            if status and "ë§ˆê°" in status.get_text(strip=True):
                print(f"â›” ë§ˆê° ìŠ¤í‚µ: {title}"); continue

            organizer = clean_organizer(extract_organizer(soup))
            date_range = extract_date_range_string(soup)
            html_content = await fetch_clean_body(url)

            results.append({
                "title": title,
                "date":  date_range,
                "host":  organizer,
                "url":   url,
                "html":  html_content,
            })
            print(f"âœ… ì™„ë£Œ: {title}")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {url} â†’ {e}")
            continue
    driver.quit()
    return pd.DataFrame(results)

# ---------------------------------------------------------------------------
# 4) main
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    LIMIT = 10       # â–¶ï¸ ìˆ«ì ì…ë ¥í•˜ë©´ ìµœì‹  nê±´ë§Œ ìˆ˜ì§‘
    df = asyncio.run(crawl_dacon(limit=LIMIT))

    if df.empty:
        print("âš ï¸  ìˆ˜ì§‘ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        os.makedirs("output", exist_ok=True)
        fname = f"dacon_competitions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        fpath = os.path.join("output", fname)
        df.to_csv(fpath, index=False, encoding="utf-8-sig")
        print(f"ğŸ‰ ì €ì¥ ì™„ë£Œ â†’ {fpath} ({len(df):,} rows)")